{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "003d3879",
   "metadata": {},
   "source": [
    "# Подготовка модели распознавания рукописных букв и цифр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17fa0c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from torchvision.datasets import EMNIST\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b085cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3366cfc4",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1bd2a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EMNIST('data/', 'balanced', download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2eeb2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABsElEQVR4nL3QS0sbURQH8P+9cyeJ5mFGK0mqpgEjiAatIGShWEuEFmoxqwqVfgDBD9Nv0Y1oWhQ3EXx0UR8gomgkqW1jNQm2SdEZJcnk3i5mElsfSz3L8+Mczv8A91Lklh4lgt+FVqdDK3B+G9K6aCSUXtle5QC7ZrIr8DL8+ImTf0YNJWOD7OganmizEFurQqpI3W8VCgDOQJ+rCfx099M2ryJxvmqTAUC2KxTF8/3V+T81FFqmvb162kUsvnz2G6giP4/LPguhFESUkwtrR/y/J9h8UQcLBTuZ9nVqsyTMJea1xZNZ2ZLo9danNlPFm++r9441NngOprOnoobmJLV0PB138OnFZOnKzEn2LPzm0dFifiZTNFriatLqHhwJXCZi+YzOQOwANKGbKL0YjTaczL1P682hAUr9BOJb4cMvA619/Yok9DxsHYPjDHUCRM0uGUgj71ok+Ma4ah/xutT8+ncSfN4c3quAAcRvJ4DUMlnSslupjS8FlXpeI14BGCD2dnoUPVfG2Y+13M/D4wonxx+RMaOwoW63tqzyslYQ/J+YRs5Gl1zO6QKc4yHqLyNnnTPTA4PRAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABXUlEQVR4nL2QvS9DYRjFf+/1Vn3cUhQh8bF1wEBiMpBgMHa1WAwi8UfYJMToXzAzmJWJBYNBIkJ9tIloSwn33rfPNfSiV7pytpPfc56cHPh/KVADZB2UDzAyfbPnVPOuo+OFZHJ2tDOi7YPX3HzjN9FYE2PWeoFm5+RcWsajTav1B1kPU3lbt7JZuRMBrXz/KbNfKqWf7j2ApCvO1dWb67quKyJlERFx71MWGsTnbik/GQNalqPOYX9bXGlt9ys0FJ87Ls8fzgAaU4PZxYbhodZJe3unjAZQiWilwbtH83vmcjfSHbn9AA3xOLYOyr/Q1vcoYq4BsEKTmLTRU/rbhiGlkKtA+bIxTNpU8568uxFMlsjK22AoaQwlL7DiXeR+oIYe2/96m1+Ln35UQz3TUMz4QXBLCaEkRDtUYMu/2pp94xR8asmCvBeba68JNeSOemNNNaEC6pRfrgn/SJ9qSIX2stwl5QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABUklEQVR4nNWQzyuDcRzHX8+z70MSm7RNLWEtLiw1NzdX+wuU/AEu4rZiKS5OjppyUQ5LIQcHBylKHKS1sZIDBz+zJ9nYs2efOWwPOezO+/b5vHt/en3e8E/kqm/p0ZOx+rkJ+7oZFLS574vOVmkBA1B9emBkF2DcnAPA2zs9f5bNW5ZlWbbIpg4QL582KsMbSV7lLRGpVCVphUJFUccteNoUIELOBCNgkLRR0KqFdSi7KN+VUmlJpYXgStfnFigA8813vjRIeq8kAuCKdZDJAKhsYTEyNdCglO4w+x6kEK/SZ2+Gfn/Zsy5y0FM1L/MzvxuIv0th2KgO8fKFX/vx/IkPeV51ug3niwm/U6oRXHgS2QkBoIE+O+m+2rZvu4GB/s4mPveXD+Xn0JNtFV9rtZUe10I1bg1AD8ZGPRrkTEhtHL04sRqJ3u4BTLOCfB/8g/oCj9CIK5+UtIQAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABvUlEQVR4nMXSS2sTURQH8P+9mcmNZJKZqS15lEKDzUKaPnDTiBsXRoqgoFI34sdw58K9H6KfQHHRXauglrYrUTNxYYWOWHRS4qTNJC3zOtdFJlofW/HsLj/On8s5B/gnxX57cw4g+jtmcgYQOmH8JypqdWaW02DDOfJ/RTZRbBSvFkwmow9vNh77ElB+mLi4cMMoqxIsXdWUrS/+T0xVlh6WhHTb6wOt2qiUdp++GyFL65caZdX1XrfW+vmFpTFRsyxKIs/dXXVOOs9WLkxkgFThyUH0/oEy7GSZ+nJ9nDvW5qFPAJ10fDo+xhBFebk+mZLhoRtIAEyMC9g2AQrAGzdvCxZTGMUSALhxXkerleBMTTDp9ve7yQ84ZyBKYvM6k71HO/u94UzNOZMFXpggEQAZdQJOAFCc1eDZgyFSq1nl+fv31r2m/bkP/c519durTTfBj9a1M+xsLhfM7W1/QqVeCh3Li5NY5+3XScEzU2QuVmw2Pa93d61gtBUmbl2p6WpBSXGiGD3v+drOAZJO6W97TSN7WTM1JWoH1t7L5tHpZXPOlEK6Np0dvPC6fX8081N3wIysGrajmPAf6zvc5LETe+Ld5AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABtUlEQVR4nK2STS8DURSGzx3X7YcOLUJVKj4WIo2PhUUtRFdSFqI7SRf+Ar8AW0t/QIK1VGgaISEaQSI0PhYq6dCQlEZqWi3TMXcs7swUZSPO5t7cJ++bc95zAf6vOM64oi/AxDu4Lkgmc1IZNNWM+LycHXLR3YgoAQAuMeTqGe92Iw4cg/bM2c1nR2TqXYnL8kMikS8q8smM9bPS1DQ+4HwX9wXw2ZuJq8+ZKOnMweWXQmpxssFiae9fe1Ry09hQkia/13x7Fjp/UiCZjnvqiU23RaTfHzALs0eCAgDv+YO2VmRM2zB2+pi/Dlr0uTrnisUZ3dbZ12FJHh2+qRqklJ0YAMyBgPVtKSSUJwkAvMcpp2IpUk1K7yrVlI4ex/16lEzV70Solr2SvaQM2ghClc11o9zdJoO2KsjfMoh9jdg1MYxdAq/1MeRDUtaITyW1terrQUzrkufFy2fWEBWeZRWodL19wSDiICe8MCWNzrcFrVux1asiY1UtFXt7+j7FcKv/deM4LmlLwDjNTDAAyInMAoQyirGi1MVaQU8IaDYMosGgQkpLKvxcnN3r/gUBfP+Sf6kP/OKg9EJKzLUAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABkElEQVR4nKWRPUgCYRjH//dhr4qWWVwtxqmjlREtQbREQ4R9bQVlU2vQ0hBU0NbU2pJDc5QtgUhzCCUlIZhEEJJZWaJW53XvNZxf1bXUf3kfnh/Px/95gb+K0UsSa7Ockr/mWE57jTM78eOur8zsW3QaAXDjiSItLvEAX+0vzs229R3tSbCOWrKCwVo/271boDQfEYGGken529IaD7AaM7nWp8xgSBMBSqG42KJSVNq2jg5PmqRHgbN5rigM4xPk4ZKWK9muiX4j7sI5xuJkgcbOdqSTtDzQtPlOaXpFDLzJQReIPy2/rhkr24gnCi1tCaw3WihsOAaiihLtrq5qX818ZP0ciH9fzoRikpz2k5oRRyibW+AAYezsTaFKIiDUuXSeFq97ATDEG7i7j8y4tZNrVniLQc4DUKWL5SAuUiW1DtakZg5RMVGGbB2mtVA7gsfG5L/9Xw06Lbgp/gI7BgmNvehD8+wQXmLKTwiA9SVpPuLSITzA9jg+wgcpfag+J+Lb55IOZADY7bknqsP+oU8O2Ix45Osa0AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABkklEQVR4nLWROy/DYRTGn/ft+/oLWvUn6hJSIgyusUmaqhCboYtBjL6Ab+Aj2ESsVoNZB4JEMDRKqEuZVFOXUC3/23kNWtoySZzx/M558pznAP9S7EeHaz5kH+hXWDUeDOF6aYsAiGLg0qvrp2bbheqLbZdDoQf8LROtAoznG99bbWMjAbcUgErfRqkAOZe+ioHp4WaNA0TW+WL0rGCIjwZqQzV1ugsA3jeiz5G4qQqC3pWc6RCRadpkr3fJbxsCUDlbOICVsnWdLpMWiuHL4o0HQGbLmp+FovLDhRBCCNZxSHTg//TOv06x8zNuwCOZSzZ7g1cb72UJAZB+z2hLyNt0u7BmlICeOJGRSGRNInL2/CWbzC0B2abKxQCgcnD1jZRSRLaRTezPaF8vc8nucLhHA0DO004ss/mYNAAGsPo63t8b7q4EoNK7R7GdJ2UX9BqWT+OpHBERWXdzjZIXJ1Qz1Mk4B2wII3n44JTEBwBE1kWETR2vnxYxMECbHOJAJnLBQ68n9yiF4DyfIceP2P9YH00UnMLyh6K5AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABxElEQVR4nLWRv2sTYRjHP/feGy4EW9L21A4KXunVgkNDC0IHi6AdRCS4C13cLTo4qO3mWAQni0aqgiCCutrBQXFRmgq2Z6WEEoWSaFpNekmTN3njcHcJ/gH9Ts/zfni+7/MDDkQGYAz07Zba3Te7D/L1MHEy3gO7y+IZb8ObcWJBMrenv410mJn2tdbFtw4Aw+u6vTdrRnDos66uLtb86xIJf9dHhJ+P/hSp478XX/w8f6wHJPj5NrtfdQitycTz+wWpACQcmTL3P+5ErkdPv779K4wlxHpYvluKYPnR+yJdKNBLuTaAEE12nkTjg0CcSuo1DRC/eLM/rEkmVQVA3qk33GDA774bQreemwhsAw8GrjidHfYaqgwCoNkErAtnTKXCTs6aFRXC/TowOnuYQtho/2TrXQFAzjcaLhzKtrT2h4LCG/7mOCCgogD72ihlZaUEYJyYslZ+BB4Tucb0yavbteytL617gwbG2LNqLR0ewvV0bqPYfJXqndkuPh5OjGVraiURtm1nfK3V5jkTe15V3yysttRymmhP8elxvfahCFgPLyfQW0+Xtjq3R0gpgsiZ++S9vBR5Gvyv+GDsT0lzoPoHVC6nJSoVE8sAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABBklEQVR4nGNgIB7kXFVA4jGhSrIqsWJoYOrKgbJEMCWZr/risYuFBHcNPqAyk4kBI4QYVBQYGBgYGB0c0CUYGBgY1vxWYuAq3PhvPjbJoD97ii/8fbVGBEOGRSHo4r//3+ars6PLMKqt//n33a23v3gwNDH63/l3p96NmXnHS4Rt0BDnOad8V+PuXwYG5m8f/qHp4zr/d70YAwMDA4PInVNockzP/vgzQpjqv9LQJBX+rYUxa39xIrmegYGBIQDK4fB66//oO5pOo3/fJJgYGPTP/6m7HY/hj/X//rxcd+ff91WaTxUwfMkUdOffvztFYgyuIRhyDAwMzAqqzAwMDKxG2CSxAgDmfktAlr90tgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABjklEQVR4nLWRS0sCURiG3zkzg+KMmUpMJYWY4CKNoAiCQIOWboPo8hv6C+36GUV/wJZFqy5kRWRWFgWWUjQtmhpMy5n0nBY6ecGt3/J9eL7LOUBXigPg6REBvfhbCwRF0DUGABAAEg73gWbyBQAAkaOO9J7B6hDO8Xk70Us1E6Ii5NeST3VIb12TA16P25okwDf3lad1M/d6E5yelQg4wSdyDNXi4BBntWXGg5pTJQLes9BLUC4cHWSYte3/ZoGZ9V7tLXFz+EGpZQIAKuD4iZhkJtOJN63afrE4ErvQUhtBRyP6N3lX3O/Vti9fjA5P5V95/thacvDNmWWSkSmW2zn9YZ3gcDyyuXtutjAL8mNR+1W20zjAFkj83IfE9rhmOkcjBIQ0RZUGlAMK711OXVOAyFGJ0NKe+m7B4qMacMZDEQoQKSYRWrSd7NN6c86+eHaXVUvfhmmaVVoxy8ervGUyI6nILn9Y7Bc4VJmu0+ssa/kVUXbLMYkANJOhn19GEwRAeEERAUDXGaXoWv0B9AuRywprHhgAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABnElEQVR4nL2RvUtbURjGf/fNuSa3EitRtEUEReOg0A4VuziktA429kMpLqX4Bzh19guhm/0v2smCuhQntahgoSUIpR8G1EIGkw4pTap477k56XATEdOO9Rnf3znP+/I8cPmyaiZ2m43OlP7yVpyOxPre3oeH4RqkWsYWPh64xpjUjXO2oabBPokm2puE06y2r6uVyR9nmx+9ybqeNqbs7s/dive/NtkxqcLOtDba844zJ5knDlg3f3sL9aAA1OMuDlcKFFNDE8klKH9buz+6+dYA4Lw0+0/rlVIycPBVAcx77oxCAOL3Tl8tH/u+L4OtwaqCbwkIoIbi6aUTAKJ2cMRhEQLY+6zuxecLYVi6CAKR0R4vFcRlRyvwdmPunY/AtZFw7iiYtSZCRQB1J5TPg0C4wdK6Enq0tOEDsfbSRg4E6WvEVNwa7J87gDMcy++4IEjvVf9TQFWiJbMLzePT3uq2AZAH7xfvBh+bF3/NRrgydVRa7q4WHapmHE4mI6jnBS/dGTgBZ627qxjK+svW5veariuKdTjyL/b/9QeZoIvYnqzh/gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(11, 22):\n",
    "    img, lbl = dataset[i]\n",
    "    print(lbl, img.size)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12721395",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EMNIST('data/', 'balanced', download=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8188171e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.EMNIST"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a80f6a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка словаря соответствий\n",
    "label_mapping = {}\n",
    "with open('emnist-balanced-mapping.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        label, char = line.split()\n",
    "        label_mapping[int(label)] = chr(int(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0200d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '0',\n",
       " 1: '1',\n",
       " 2: '2',\n",
       " 3: '3',\n",
       " 4: '4',\n",
       " 5: '5',\n",
       " 6: '6',\n",
       " 7: '7',\n",
       " 8: '8',\n",
       " 9: '9',\n",
       " 10: 'A',\n",
       " 11: 'B',\n",
       " 12: 'C',\n",
       " 13: 'D',\n",
       " 14: 'E',\n",
       " 15: 'F',\n",
       " 16: 'G',\n",
       " 17: 'H',\n",
       " 18: 'I',\n",
       " 19: 'J',\n",
       " 20: 'K',\n",
       " 21: 'L',\n",
       " 22: 'M',\n",
       " 23: 'N',\n",
       " 24: 'O',\n",
       " 25: 'P',\n",
       " 26: 'Q',\n",
       " 27: 'R',\n",
       " 28: 'S',\n",
       " 29: 'T',\n",
       " 30: 'U',\n",
       " 31: 'V',\n",
       " 32: 'W',\n",
       " 33: 'X',\n",
       " 34: 'Y',\n",
       " 35: 'Z',\n",
       " 36: 'a',\n",
       " 37: 'b',\n",
       " 38: 'd',\n",
       " 39: 'e',\n",
       " 40: 'f',\n",
       " 41: 'g',\n",
       " 42: 'h',\n",
       " 43: 'n',\n",
       " 44: 'q',\n",
       " 45: 'r',\n",
       " 46: 't'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88d9d636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset EMNIST\n",
       "    Number of datapoints: 112800\n",
       "    Root location: data/\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25bf0501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер тренировочной выборки: 78960\n",
      "Размер валидационной выборки: 22560\n",
      "Размер тестовой выборки: 11280\n"
     ]
    }
   ],
   "source": [
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "train, val, test = random_split(dataset, [0.7, 0.2, 0.1], generator=generator)\n",
    "print(f'Размер тренировочной выборки: {train.__len__()}')\n",
    "print(f'Размер валидационной выборки: {val.__len__()}')\n",
    "print(f'Размер тестовой выборки: {test.__len__()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ce2271",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee8e0346",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 10\n",
    "lr = 1e-3\n",
    "m = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36030057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "dataloader_train = DataLoader(\n",
    "    train, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "dataloader_val = DataLoader(\n",
    "    val, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "dataloader_test = DataLoader(\n",
    "    test, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b459446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        self.conv_1 = nn.Conv2d(\n",
    "            in_channels=in_ch,\n",
    "            out_channels=out_ch,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.conv_2 = nn.Conv2d(\n",
    "            in_channels=out_ch,\n",
    "            out_channels=out_ch,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.activation(self.bn1(self.conv_1(x)))\n",
    "        y = self.bn2(self.conv_2(y))\n",
    "        return self.activation(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9ec2cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMNIST_CNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.conv_2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.block_1 = ConvBlock(32, 32)\n",
    "        self.pooling_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.block_2 = ConvBlock(32, 32)\n",
    "        self.pooling_2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        self.flat = nn.Flatten()\n",
    "        self.linear_1 = nn.Linear(1152, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.linear_2 = nn.Linear(512, 47)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv_1(x) # 16, 28, 28\n",
    "        y = self.conv_2(y) # 32, 28, 28\n",
    "        y = self.pooling_1(self.block_1(y)) \n",
    "        y = self.pooling_2(self.block_2(y)) \n",
    "\n",
    "        y = self.flat(y)\n",
    "        y = self.activation(self.linear_1(y))\n",
    "        y = self.bn1(y)\n",
    "        y = self.activation(self.linear_2(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73deda34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels, preds):\n",
    "    predictions = torch.argmax(preds, dim=1)\n",
    "    correct = (labels == predictions).sum().numpy()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e160426",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EMNIST_CNN()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = model.to(device)\n",
    "loss_fn = loss_fn.to(device)\n",
    "\n",
    "opimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a7521ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1; loss: 0.8755571210374701\n",
      "accuracy on validation: 0.8508966619318182\n",
      "Epoch 2; loss: 0.4230241534532035\n",
      "accuracy on validation: 0.8691850142045454\n",
      "Epoch 3; loss: 0.3636764312965156\n",
      "accuracy on validation: 0.8757102272727273\n",
      "Epoch 4; loss: 0.3327684442105568\n",
      "accuracy on validation: 0.8780628551136364\n",
      "Epoch 5; loss: 0.3110421671496594\n",
      "accuracy on validation: 0.876953125\n",
      "Epoch 6; loss: 0.2937392464749038\n",
      "accuracy on validation: 0.8843661221590909\n",
      "Epoch 7; loss: 0.2798436731039656\n",
      "accuracy on validation: 0.8854758522727273\n",
      "Epoch 8; loss: 0.26834248622693574\n",
      "accuracy on validation: 0.8857865767045454\n",
      "Epoch 9; loss: 0.2587197158555141\n",
      "accuracy on validation: 0.8814364346590909\n",
      "Epoch 10; loss: 0.2509949989344952\n",
      "accuracy on validation: 0.8859641335227273\n"
     ]
    }
   ],
   "source": [
    "best_mean_accuracy = 0\n",
    "for epoch in range(epochs):\n",
    "    loss_val = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for img, label in dataloader_train:\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        opimizer.zero_grad()\n",
    "        label = nn.functional.one_hot(label, 47).float()\n",
    "        pred = model.forward(img)\n",
    "        loss = loss_fn(pred, label)\n",
    "        loss.backward()\n",
    "        opimizer.step()\n",
    "        loss_val += loss.item()\n",
    "    print(f'Epoch {epoch+1}; loss: {loss_val / len(dataloader_train)}')\n",
    "\n",
    "    model.eval()\n",
    "    accuracies = []\n",
    "\n",
    "    for img, label in dataloader_val:\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        y_pred = model(img)\n",
    "        accuracies.append(accuracy(label.cpu(), y_pred.cpu()))\n",
    "\n",
    "    mean_accuracy = np.mean(np.array(accuracies))\n",
    "    if mean_accuracy > best_mean_accuracy:\n",
    "        best_mean_accuracy = mean_accuracy\n",
    "        torch.save(model, 'EMNIST_CNN_best.pth')\n",
    "    \n",
    "    print(f'accuracy on validation: {np.mean(np.array(accuracies))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da5312f",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4424e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = torch.load('myapp/model.ckpt')\n",
    "trained_model = EMNIST_CNN()\n",
    "trained_model.load_state_dict(model_ckpt['model_state_dict'])\n",
    "trained_model.eval()\n",
    "trained_model = trained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b496b534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test: 0.8859641335227273\n"
     ]
    }
   ],
   "source": [
    "val_acc = []\n",
    "for img, label in dataloader_val:\n",
    "    img = img.to(device)\n",
    "    label = label.to(device)\n",
    "    y_pred = trained_model(img)\n",
    "    val_acc.append(accuracy(label.cpu(), y_pred.cpu()))\n",
    "print(f'accuracy on test: {np.mean(np.array(val_acc))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d18c0c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test: 0.8858309659090909\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "for img, label in dataloader_test:\n",
    "    img = img.to(device)\n",
    "    label = label.to(device)\n",
    "    y_pred = trained_model(img)\n",
    "    test_acc.append(accuracy(label.cpu(), y_pred.cpu()))\n",
    "print(f'accuracy on test: {np.mean(np.array(test_acc))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60145f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save({'model_state_dict': trained_model.state_dict()}, os.path.join('myapp', 'model.ckpt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
